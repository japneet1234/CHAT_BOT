{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9785c7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder structure created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dirs = [\n",
    "    \"data/processed\",\n",
    "    \"data/vocab\",\n",
    "    \"src\"\n",
    "]\n",
    "\n",
    "for d in base_dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"✅ Folder structure created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83530f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "url = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "output_path = \"data/raw/cornell.zip\"\n",
    "extract_path = \"data/raw/cornell\"\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# Download\n",
    "if not os.path.exists(output_path):\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(\"✅ Download complete.\")\n",
    "\n",
    "# Extract\n",
    "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "    print(\"✅ Extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "272f502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total pairs: 221282\n",
      "Example:\n",
      "👤 Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "🤖 Well, I thought we'd start with pronunciation, if that's okay with you.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load movie lines\n",
    "lines_path = Path(extract_path) / \"cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "convo_path = Path(extract_path) / \"cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "\n",
    "# Map line IDs to text\n",
    "id2line = {}\n",
    "with open(lines_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 5:\n",
    "            line_id, text = parts[0], parts[4]\n",
    "            id2line[line_id] = text\n",
    "\n",
    "# Get conversation sequences\n",
    "conversations = []\n",
    "with open(convo_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 4:\n",
    "            line_ids_str = parts[3]\n",
    "            line_ids = eval(line_ids_str)\n",
    "            conversations.append(line_ids)\n",
    "\n",
    "# Create pairs: (input, response)\n",
    "pairs = []\n",
    "for conv in conversations:\n",
    "    for i in range(len(conv) - 1):\n",
    "        first = id2line.get(conv[i], \"\").strip()\n",
    "        second = id2line.get(conv[i+1], \"\").strip()\n",
    "        if first and second:\n",
    "            pairs.append((first, second))\n",
    "\n",
    "print(f\"✅ Total pairs: {len(pairs)}\")\n",
    "print(\"Example:\")\n",
    "print(\"👤\", pairs[0][0])\n",
    "print(\"🤖\", pairs[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "81a777e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 221282 dialogue pairs\n",
      "✅ Tokenizer trained and saved\n",
      "✅ Encoded 1000 pairs\n",
      "✅ Saved train, val, and test splits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "\n",
    "# === CONFIG ===\n",
    "lines_path = \"data/raw/cornell/cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "convo_path = \"data/raw/cornell/cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "vocab_dir = \"data/vocab\"\n",
    "out_dir = \"data/processed\"\n",
    "vocab_size = 30000\n",
    "\n",
    "# === CLEAN FUNCTION ===\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s\\.\\?!']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# === LOAD LINES ===\n",
    "id2line = {}\n",
    "with open(lines_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 5:\n",
    "            line_id, text = parts[0], parts[4]\n",
    "            id2line[line_id] = clean(text)\n",
    "\n",
    "# === LOAD CONVERSATIONS ===\n",
    "conversations = []\n",
    "with open(convo_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" +++$+++ \")\n",
    "        if len(parts) == 4:\n",
    "            ids = eval(parts[3])\n",
    "            conversations.append(ids)\n",
    "\n",
    "# === CREATE (input, response) PAIRS ===\n",
    "pairs = []\n",
    "for conv in conversations:\n",
    "    for i in range(len(conv) - 1):\n",
    "        first = id2line.get(conv[i], \"\")\n",
    "        second = id2line.get(conv[i + 1], \"\")\n",
    "        if first and second:\n",
    "            pairs.append((first, second))\n",
    "\n",
    "print(f\"✅ Loaded {len(pairs)} dialogue pairs\")\n",
    "pairs= pairs[:1000]  # Limit to first 1000 pairs for testing\n",
    "\n",
    "# === TRAIN TOKENIZER ===\n",
    "def train_tokenizer(pairs, vocab_size, save_path):\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"]\n",
    "    )\n",
    "\n",
    "    all_texts = [s for p in pairs for s in p]\n",
    "    tokenizer.train_from_iterator(all_texts, trainer=trainer)\n",
    "\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"[SOS] $A [EOS]\",\n",
    "        pair=\"[SOS] $A [EOS] [SOS] $B [EOS]\",\n",
    "        special_tokens=[\n",
    "            (\"[SOS]\", tokenizer.token_to_id(\"[SOS]\")),\n",
    "            (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # tokenizer.decoder = decoders.BPE()\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "    tokenizer.save(str(Path(save_path) / \"chatbot_tokenizer.json\"))\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = train_tokenizer(pairs, vocab_size, vocab_dir)\n",
    "print(\"✅ Tokenizer trained and saved\")\n",
    "\n",
    "# === ENCODE & SPLIT ===\n",
    "def encode_pairs(pairs, tokenizer):\n",
    "    encoded = []\n",
    "    for src, tgt in pairs:\n",
    "        src_ids = tokenizer.encode(src).ids\n",
    "        tgt_ids = tokenizer.encode(tgt).ids\n",
    "        encoded.append((src_ids, tgt_ids))\n",
    "    return encoded\n",
    "\n",
    "encoded_pairs = encode_pairs(pairs, tokenizer)\n",
    "print(f\"✅ Encoded {len(encoded_pairs)} pairs\")\n",
    "\n",
    "# === SPLIT DATA ===\n",
    "train, temp = train_test_split(encoded_pairs, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "torch.save(train, f\"{out_dir}/train.pt\")\n",
    "torch.save(val, f\"{out_dir}/val.pt\")\n",
    "torch.save(test, f\"{out_dir}/test.pt\")\n",
    "\n",
    "print(\"✅ Saved train, val, and test splits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f4054112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 Input  1: well no ...\n",
      "🤖 Output 1: then that ' s all you had to say .\n",
      "------------------------------------------------------------\n",
      "👤 Input  2: but we do have a lack of notaries . you should contact my administration .\n",
      "🤖 Output 2: don bobadilla is already a judge my dear don cristobal .\n",
      "------------------------------------------------------------\n",
      "👤 Input  3: so ... the station is empty ?\n",
      "🤖 Output 3: yeah . this way .\n",
      "------------------------------------------------------------\n",
      "👤 Input  4: utapan won ' t you speak to me ? you used to know how to speak to me .\n",
      "🤖 Output 4: you never learned how to speak my language .\n",
      "------------------------------------------------------------\n",
      "👤 Input  5: she just saw two of her friends killed ! they probably threatened her .\n",
      "🤖 Output 5: is that all there is ?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\japne\\AppData\\Local\\Temp\\ipykernel_8200\\2022665501.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(\"data/processed/train.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"data/vocab/chatbot_tokenizer.json\")\n",
    "\n",
    "# Load sample data\n",
    "dataset = torch.load(\"data/processed/train.pt\")\n",
    "\n",
    "# Function to decode token IDs\n",
    "def decode_ids(ids):\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "# Show 5 samples\n",
    "for i in range(5):\n",
    "    input_ids, output_ids = dataset[i]\n",
    "    input_text = decode_ids(input_ids)\n",
    "    output_text = decode_ids(output_ids)\n",
    "    \n",
    "    print(f\"👤 Input  {i+1}: {input_text}\")\n",
    "    print(f\"🤖 Output {i+1}: {output_text}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "30f0db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟦 Sample 1\n",
      "🔢 Input IDs : [1, 228, 57, 88, 2]\n",
      "🗣️  Input Text: well no ...\n",
      "🔢 Output IDs: [1, 203, 76, 5, 35, 106, 45, 281, 53, 162, 6, 2]\n",
      "🤖 Output Text: then that ' s all you had to say .\n",
      "--------------------------------------------------------------------------------\n",
      "🟦 Sample 2\n",
      "🔢 Input IDs : [1, 135, 67, 77, 117, 17, 1802, 79, 2935, 6, 45, 357, 1555, 100, 3699, 6, 2]\n",
      "🗣️  Input Text: but we do have a lack of notaries . you should contact my administration .\n",
      "🔢 Output IDs: [1, 99, 1156, 55, 505, 17, 2306, 100, 2061, 99, 2570, 6, 2]\n",
      "🤖 Output Text: don bobadilla is already a judge my dear don cristobal .\n",
      "--------------------------------------------------------------------------------\n",
      "🟦 Sample 3\n",
      "🔢 Input IDs : [1, 82, 88, 51, 1225, 55, 2262, 16, 2]\n",
      "🗣️  Input Text: so ... the station is empty ?\n",
      "🔢 Output IDs: [1, 183, 6, 131, 169, 6, 2]\n",
      "🤖 Output Text: yeah . this way .\n",
      "--------------------------------------------------------------------------------\n",
      "🟦 Sample 4\n",
      "🔢 Input IDs : [1, 2527, 340, 5, 36, 45, 399, 53, 54, 16, 45, 535, 53, 112, 143, 53, 399, 53, 54, 6, 2]\n",
      "🗣️  Input Text: utapan won ' t you speak to me ? you used to know how to speak to me .\n",
      "🔢 Output IDs: [1, 45, 222, 3432, 143, 53, 399, 100, 2472, 6, 2]\n",
      "🤖 Output Text: you never learned how to speak my language .\n",
      "--------------------------------------------------------------------------------\n",
      "🟦 Sample 5\n",
      "🔢 Input IDs : [1, 151, 142, 847, 415, 79, 122, 754, 850, 4, 171, 1157, 982, 122, 6, 2]\n",
      "🗣️  Input Text: she just saw two of her friends killed ! they probably threatened her .\n",
      "🔢 Output IDs: [1, 55, 76, 106, 178, 55, 16, 2]\n",
      "🤖 Output Text: is that all there is ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\japne\\AppData\\Local\\Temp\\ipykernel_8200\\51510444.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(\"data/processed/train.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"data/vocab/chatbot_tokenizer.json\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = torch.load(\"data/processed/train.pt\")\n",
    "\n",
    "# Print 5 samples\n",
    "for i in range(5):\n",
    "    input_ids, output_ids = dataset[i]\n",
    "    \n",
    "    # Decode\n",
    "    input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"🟦 Sample {i+1}\")\n",
    "    print(\"🔢 Input IDs :\", input_ids)\n",
    "    print(\"🗣️  Input Text:\", input_text)\n",
    "    print(\"🔢 Output IDs:\", output_ids)\n",
    "    print(\"🤖 Output Text:\", output_text)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6cd3964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 → [PAD]\n",
      "  1 → [SOS]\n",
      "  2 → [EOS]\n",
      "  3 → [UNK]\n",
      "  4 → !\n",
      "  5 → '\n",
      "  6 → .\n",
      "  7 → 0\n",
      "  8 → 1\n",
      "  9 → 2\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])  # sort by ID\n",
    "for token, idx in sorted_vocab[:10]:  # top 10 tokens\n",
    "    print(f\"{idx:>3} → {token}\")\n",
    "\n",
    "tokenizer.token_to_id(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e27aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4191b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Validation set size: 100\n",
      "Test set size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\japne\\AppData\\Local\\Temp\\ipykernel_27128\\477582321.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(\"data/processed/train.pt\")\n",
      "C:\\Users\\japne\\AppData\\Local\\Temp\\ipykernel_27128\\477582321.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_data = torch.load(\"data/processed/val.pt\")\n",
      "C:\\Users\\japne\\AppData\\Local\\Temp\\ipykernel_27128\\477582321.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"data/processed/test.pt\")\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load(\"data/processed/train.pt\")\n",
    "val_data = torch.load(\"data/processed/val.pt\")\n",
    "test_data = torch.load(\"data/processed/test.pt\")\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "print(\"Test set size:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e63c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len_input = max(len(x) for x, _ in train_data)\n",
    "max_len_target = max(len(y) for _, y in train_data)\n",
    "\n",
    "print(\"Max length of input:\", max_len_input)\n",
    "print(\"Max length of target:\", max_len_target)\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8c9c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, data, pad_token_id, max_len=40):\n",
    "        self.data = data\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        if len(src) > self.max_len:\n",
    "            src = src[:self.max_len]\n",
    "        if len(tgt) > self.max_len:\n",
    "            tgt = tgt[:self.max_len]\n",
    "\n",
    "        src = src[:self.max_len] + [self.pad_token_id] * (self.max_len - len(src))\n",
    "        tgt = tgt[:self.max_len] + [self.pad_token_id] * (self.max_len - len(tgt))\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dd3a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        _, (hidden, cell) = self.lstm(embed)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "776ed0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        embed = self.embedding(x)\n",
    "        out, (hidden, cell) = self.lstm(embed, (hidden, cell))\n",
    "        logits = self.fc(out.squeeze(1))\n",
    "        return logits, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dffb089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_token_id, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input_token = tgt[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = tgt[:, t] if random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ddd4e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "train_dataset = train_dataset.remove_columns(\n",
    "    [col for col in train_dataset.column_names if col not in ['input_ids', 'labels']]\n",
    ")\n",
    "val_dataset = val_dataset.remove_columns(\n",
    "    [col for col in val_dataset.column_names if col not in ['input_ids', 'labels']]\n",
    ")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4809900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 9025\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28224610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "# pad_id = tokenizer.token_to_id(\"[PAD]\")  # you should have this from preprocessing\n",
    "# vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Datasets\n",
    "# train_dataset = ChatbotDataset(train_data, pad_id)\n",
    "# val_dataset = ChatbotDataset(val_data, pad_id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model + optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder, pad_id, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a01bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(train_data[0][0])\n",
    "print(pad_id)\n",
    "print(train_dataset[1][0], train_dataset[1][1])  # Check shape of first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # for src, tgt in loader:\n",
    "    for batch in loader:\n",
    "        src, tgt = batch['input_ids'], batch['labels']\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output[:, 1:].reshape(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch} | Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "862d2d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\japne\\AppData\\Local\\Temp\\ipykernel_21012\\656327292.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# Redefine the encoder and decoder\n",
    "tokenizer = Tokenizer.from_file(\"data/vocab/chatbot_tokenizer.json\")\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "# Recreate the Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder, pad_id, device).to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cd1d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: what ' s normal ?\n",
      "Output Text: bogey lowenstein ' s party is normal but you ' re too busy listening to bitches who need prozac to know that .\n",
      "Generating response...\n",
      "i ' m getting trashed man . isn ' t that what you ' re supposed to do at a party ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, input_text, max_len=40):\n",
    "    model.eval()\n",
    "    ids = tokenizer.encode(input_text).ids\n",
    "    ids = ids[:max_len] + [pad_id] * (max_len - len(ids))\n",
    "    src = torch.tensor(ids).unsqueeze(0).to(device)\n",
    "\n",
    "    hidden, cell = model.encoder(src)\n",
    "    input_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")]).to(device)\n",
    "\n",
    "    output_ids = []\n",
    "    for _ in range(max_len):\n",
    "        output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "        top1 = output.argmax(1).item()\n",
    "        if top1 == tokenizer.token_to_id(\"[EOS]\"):\n",
    "            break\n",
    "        output_ids.append(top1)\n",
    "        input_token = torch.tensor([top1]).to(device)\n",
    "\n",
    "    # print(output_ids)\n",
    "    # print([tokenizer.id_to_token(id) for id in output_ids])\n",
    "    # print(tokenizer.decode(output_ids))\n",
    "\n",
    "    # return tokenizer.decode(output_ids)\n",
    "    return output_ids\n",
    "\n",
    "# Try it\n",
    "\n",
    "input_text = tokenizer.decode(test_data[19][0], skip_special_tokens=True)\n",
    "print(\"Input Text:\", input_text)\n",
    "output_text = tokenizer.decode(test_data[19][1], skip_special_tokens=True)\n",
    "print(\"Output Text:\", output_text)\n",
    "print(\"Generating response...\")\n",
    "response_ids = generate_response(model, tokenizer, input_text)\n",
    "for id in response_ids:\n",
    "    print(tokenizer.id_to_token(id), end=\" \")\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "779f2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9b0ff77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, tokenizer, pad_id):\n",
    "    model.eval()\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    bleu_scores = []\n",
    "\n",
    "    chencherry = SmoothingFunction()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            batch_size, seq_len = tgt.size()\n",
    "\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)  # no teacher forcing\n",
    "            predictions = output.argmax(-1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_seq = predictions[i].tolist()\n",
    "                true_seq = tgt[i].tolist()\n",
    "\n",
    "                # Remove padding and special tokens\n",
    "                pred_seq = [tok for tok in pred_seq if tok != pad_id and tok != tokenizer.token_to_id(\"[SOS]\")]\n",
    "                true_seq = [tok for tok in true_seq if tok != pad_id and tok != tokenizer.token_to_id(\"[SOS]\")]\n",
    "\n",
    "                # Token Accuracy\n",
    "                min_len = min(len(pred_seq), len(true_seq))\n",
    "                total_tokens += min_len\n",
    "                correct_tokens += sum([1 for p, t in zip(pred_seq, true_seq) if p == t])\n",
    "\n",
    "                # BLEU Score\n",
    "                ref = [tokenizer.decode(true_seq).split()]\n",
    "                hyp = tokenizer.decode(pred_seq).split()\n",
    "                bleu = sentence_bleu(ref, hyp, smoothing_function=chencherry.method1)\n",
    "                bleu_scores.append(bleu)\n",
    "\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    return token_accuracy, avg_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a3bde425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Token Accuracy: 3.87%\n",
      "Validation BLEU Score: 0.0112\n"
     ]
    }
   ],
   "source": [
    "val_accuracy, val_bleu = evaluate(model, val_loader, tokenizer, pad_id)\n",
    "print(f\"Validation Token Accuracy: {val_accuracy:.2%}\")\n",
    "print(f\"Validation BLEU Score: {val_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c9f943c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Token Accuracy: 4.37%\n",
      "Test BLEU Score: 0.0105\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ChatbotDataset(test_data, pad_id)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "test_accuracy, test_bleu = evaluate(model, test_loader, tokenizer, pad_id)\n",
    "print(f\"Test Token Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"Test BLEU Score: {test_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.0.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\japne\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 991.5/991.5 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\japne\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89720b8b",
   "metadata": {},
   "source": [
    "Using T5 tokenizer + Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c68d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047bedb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello!\n",
      "Bot: chat: Hello!\n",
      "\n",
      "User: Tell me a joke.\n",
      "Bot: chat: Tell me a joke.\n",
      "\n",
      "User: What is your name?\n",
      "Bot: chat: What is your name?\n"
     ]
    }
   ],
   "source": [
    "def chat_with_t5(input_text, max_length=50):\n",
    "    input_ids = tokenizer_t5.encode(\"chat: \" + input_text, return_tensors=\"pt\")\n",
    "    output_ids = model_t5.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    return tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the T5 model with some sample inputs\n",
    "print(\"User: Hello!\")\n",
    "print(\"Bot:\", chat_with_t5(\"Hello!\"))\n",
    "\n",
    "\n",
    "print(\"\\nUser: Tell me a joke.\")\n",
    "print(\"Bot:\", chat_with_t5(\"Tell me a joke.\"))\n",
    "\n",
    "print(\"\\nUser: What is your name?\")\n",
    "print(\"Bot:\", chat_with_t5(\"What is your name?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5485b",
   "metadata": {},
   "source": [
    "Fine tuning it on Cornell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35bcb8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a53307cec364071ab03191c4f611f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/221282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Example dialogue pairs\n",
    "# Replace this with your real (input, response) pairs\n",
    "# data = [\n",
    "#     {\"input\": \"chat: Hello!\", \"output\": \"Hi there!\"},\n",
    "#     {\"input\": \"chat: How are you?\", \"output\": \"I'm doing well, thank you.\"},\n",
    "#     {\"input\": \"chat: What's your name?\", \"output\": \"I'm a chatbot.\"}\n",
    "#     # Add more pairs from Cornell/PersonaChat here\n",
    "# ]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "data= pairs \n",
    "# Clean the data\n",
    "data = [(clean_text(src), clean_text(tgt)) for src, tgt in data]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['input', 'output']\n",
    "df['input'] = \"chat: \" + df['input']  # Prefix for T5\n",
    "df['output'] = df['output']  # No prefix for output\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "def preprocess(example):\n",
    "    input_enc = tokenizer(example['input'], truncation=True, padding='max_length', max_length=32)\n",
    "    target_enc = tokenizer(example['output'], truncation=True, padding='max_length', max_length=32)\n",
    "    input_enc['labels'] = target_enc['input_ids']\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4df57601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76327b54b5b0406c8fbef2719c300cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/221282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved to data/processed/cornell_t5_dataset\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenized dataset\n",
    "output_dir = \"data/processed/cornell_t5_dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tokenized_dataset.save_to_disk(output_dir)\n",
    "print(f\"Tokenized dataset saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29efa375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'chat: can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad. again.', 'output': 'well i thought we d start with pronunciation if that s okay with you.', 'input_ids': [3582, 10, 54, 62, 143, 48, 1704, 58, 3, 12907, 4515, 3, 5543, 9249, 11, 11, 60, 210, 18595, 17, 17, 33, 578, 46, 3, 5828, 21315, 989, 1162, 452, 1733, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [168, 3, 23, 816, 62, 3, 26, 456, 28, 30637, 3, 99, 24, 3, 7, 8957, 28, 25, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "['input', 'output', 'input_ids', 'attention_mask', 'labels']\n",
      "chat: can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad. again.\n",
      "[168, 3, 23, 816, 62, 3, 26, 456, 28, 30637, 3, 99, 24, 3, 7, 8957, 28, 25, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "chat: can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break</s>\n",
      "well i thought we d start with pronunciation if that s okay with you.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# Show the dataset along with cols\n",
    "print(tokenized_dataset[0])\n",
    "print(tokenized_dataset.column_names)\n",
    "print(tokenized_dataset[0]['input'])  # Check the input\n",
    "print(tokenized_dataset[0]['labels'])  # Check the labels\n",
    "print(tokenizer.decode(tokenized_dataset[0]['input_ids']))  # Decode the input IDs\n",
    "print(tokenizer.decode(tokenized_dataset[0]['labels']))  # Decode the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eeafde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 141620\n",
      "Validation dataset size: 35405\n",
      "Test dataset size: 44257\n"
     ]
    }
   ],
   "source": [
    "train_test = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_val = train_test['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_val['train']\n",
    "val_dataset = train_val['test']\n",
    "test_dataset = train_test['test']\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f289f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat: that is not true mrs. ayala. your route is compromised. perhaps it is time for me to deal with other distributors in california.\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a5bc124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat: can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad. again.\n",
      "well, i thought we d start with pronunciation, if that s okay with you.\n",
      "[3582, 10, 54, 62, 143, 48, 1704, 58, 3, 12907, 4515, 3, 5543, 9249, 11, 11, 60, 210, 18595, 17, 17, 33, 578, 46, 3, 5828, 21315, 989, 1162, 452, 1733, 1]\n",
      "[168, 6, 3, 23, 816, 62, 3, 26, 456, 28, 30637, 6, 3, 99, 24, 3, 7, 8957, 28, 25, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Input length: 32\n",
      "Output length: 32\n",
      "Input length: 136\n",
      "Output length: 71\n"
     ]
    }
   ],
   "source": [
    "# show a sample\n",
    "print(tokenized_dataset[0]['input'])\n",
    "print(tokenized_dataset[0]['output'])\n",
    "print(tokenized_dataset[0]['input_ids'])\n",
    "print(tokenized_dataset[0]['labels'])\n",
    "\n",
    "# print lengths\n",
    "print(\"Input length:\", len(tokenized_dataset[0]['input_ids']))\n",
    "print(\"Output length:\", len(tokenized_dataset[0]['labels']))\n",
    "print(\"Input length:\", len(tokenized_dataset[0]['input']))\n",
    "print(\"Output length:\", len(tokenized_dataset[0]['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c408aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\japne\\AppData\\Local\\Temp\\ipykernel_8200\\3841112317.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-chatbot\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bda244",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"t5-chatbot-model\")\n",
    "tokenizer.save_pretrained(\"t5-chatbot-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df901d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: chat: it s that woman from the coach!\n",
      "Input IDs: [3582, 10, 34, 3, 7, 24, 2335, 45, 8, 3763, 55, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Output IDs: [3, 23, 3, 195, 36, 17227, 15, 26, 233, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Output Text: i ll be damned...\n",
      "Decoded Input Text: chat: it s that woman from the coach!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Decoded Output Text: i ll be damned...</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Chatbot Response: i m not a coach.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-chatbot-model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-chatbot-model\")\n",
    "\n",
    "def chat(prompt):\n",
    "    input_text = \"chat: \" + prompt\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Try chatting\n",
    "# print(chat(\"Hello! How are you?\"))\n",
    "# print(chat(\"Tell me a joke.\"))\n",
    "# print(chat(\"What is your name?\"))\n",
    "# print(chat(\"What is the weather like today?\"))\n",
    "# print(chat(\"Can you tell me a story?\"))\n",
    "\n",
    "# Use this on test set\n",
    "\n",
    "input_text = test_dataset[110]['input']\n",
    "input_ids = test_dataset[110]['input_ids']\n",
    "output_ids = test_dataset[110]['labels']\n",
    "output_text = test_dataset[110]['output']\n",
    "\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Output IDs:\", output_ids)\n",
    "print(\"Output Text:\", output_text)\n",
    "print(\"Decoded Input Text:\", tokenizer.decode(input_ids))\n",
    "print(\"Decoded Output Text:\", tokenizer.decode(output_ids))\n",
    "print(\"Chatbot Response:\", chat(input_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1500a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:59<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: chat: that is not true mrs. ayala. your route is compromised. perhaps it is time for me to deal with other distributors in california.\n",
      "Expected: i don t think you re going to do that.\n",
      "Generated: i don t know. i don t know. i don t know. i don t know.\n",
      "\n",
      "Input: chat: this is a total catastrastroke. as of this moment i am stumped i admit i am stumped and treed both the hound dogs have me surrounded.\n",
      "Expected: poor thing don t cry rose. i know you feel awful but don t cry honey nobody s perfect. who s the father dear?\n",
      "Generated: i m stumped and treed.\n",
      "\n",
      "Input: chat: because i m tired of not understanding things. cops mafia and butlers forcing me to bust my ass to steal something which it turns out i really didn t steal it s fucked up.\n",
      "Expected: you re not thinking of going to...\n",
      "Generated: i m tired of not understanding things.\n",
      "\n",
      "Input: chat: which one is me? the horse?\n",
      "Expected: get out of here.\n",
      "Generated: i m a horse. i m a horse.\n",
      "\n",
      "Input: chat: stop this now. i ll do it. i swear.\n",
      "Expected: do something...\n",
      "Generated: i ll do it. i ll do it.\n",
      "\n",
      "Input: chat: the bridge is fine taylor how are things down there?\n",
      "Expected: transmit the code 3 emergency and take us offline. override the airlock sensors and hurry! we re running out of time we have to get to the pods!\n",
      "Generated: i don t know if i m a taylor. i don t know.\n",
      "\n",
      "Input: chat: fuck off.\n",
      "Expected: and you were obviously getting nowhere with her. i was waiting for the right moment to tell you that.\n",
      "Generated: i don t know.\n",
      "\n",
      "Input: chat: and look at this. russell came around on the trade bill.\n",
      "Expected: you re kidding.\n",
      "Generated: i m not a trader. i m a trader.\n",
      "\n",
      "Input: chat: all right. the check will be certified. want my fingerprints?\n",
      "Expected: no thanks i ve still got those. well i ll step into some working clothes and hop over to the press room for the background on this yarn. it ll be kind of fun to see the boys again too. remember bruce it must be certified.\n",
      "Generated: what do you think?\n",
      "\n",
      "Input: chat: that s nice. my parents were always traveling they weren t around so much.\n",
      "Expected: where d you grow up?\n",
      "Generated: i m glad you didn t know. i m glad you didn t know.\n",
      "\n",
      "Input: chat: this man did you see him?\n",
      "Expected: no. her husband she says. alice tried to warn us. a trap. tell mother beware. tell father that s all.\n",
      "Generated: i m not sure if i d like to see him.\n",
      "\n",
      "Input: chat: no no. the three hundred is for you doing it to me.\n",
      "Expected: what?\n",
      "Generated: i don t know. i don t know. i don t know.\n",
      "\n",
      "Input: chat: brookfield just came in.\n",
      "Expected: oh the poet? where?\n",
      "Generated: i m sorry. i m sorry.\n",
      "\n",
      "Input: chat: hold on hold on hold on\n",
      "Expected: well i m gonna hold on but you went to win this election you better change the subject. you wanna change this subject you better have a war. what do you need? it s gotta be u quick u it s gotta be u dramatic u you got to have an u enemy u . okay? what do you need in an enemy? somebody you fear. who do you fear? som b y you don t know.\n",
      "Generated: what do you think?\n",
      "\n",
      "Input: chat: now come j.j. that s a little too harsh. anyone seems fair game for you tonight.\n",
      "Expected: this man is not for you harvey and you shouldn t be seen with him in public. because that s another part of a press agents life he digs up scandal among prominent men and shovels it thin among the columnists who give him space.\n",
      "Generated: i don t know what i m saying.\n",
      "\n",
      "Input: chat: can t you give it to me later?\n",
      "Expected: yeah i u could u but the thing is if later got here sooner it would be...better.\n",
      "Generated: i can t give it to you.\n",
      "\n",
      "Input: chat: well you don t see that everyday. somebody tell me what s the deal with frida kahlo here?\n",
      "Expected: just a homeless woman. wrong place.\n",
      "Generated: i don t know what s the deal with kahlo.\n",
      "\n",
      "Input: chat: she s getting big. getting her own ideas.\n",
      "Expected: i know. well that s all i really wanted to say. so okay then.\n",
      "Generated: i m a big girl.\n",
      "\n",
      "Input: chat: well elena and i have kind of been talking not really talking but\n",
      "Expected: your mother god bless her stood by me for forty two years we never once contemplated divorce i assume you re talking here about divorce? the very thought\n",
      "Generated: i don t know what i m talking about.\n",
      "\n",
      "Input: chat: but why gray? why?\n",
      "Expected: because it would be a hurt to me to see you no more toddy. you re a pleasure to me.\n",
      "Generated: i don t know. i don t know.\n",
      "\n",
      "Input: chat: no way. she had a guy i da known believe me. sewing was her life she was really great at it. poor freddie.\n",
      "Expected: did you ever work with her?\n",
      "Generated: i don t know. i don t know. i don t know.\n",
      "\n",
      "Input: chat: hey i m eating.\n",
      "Expected: i m serious wade. this case is important to me.\n",
      "Generated: i m not eating.\n",
      "\n",
      "Input: chat: great. hey. that s great.\n",
      "Expected: listen nick\n",
      "Generated: i m not sure what i ll do.\n",
      "\n",
      "Input: chat: flynn! i made it. i worked out some new codes for tron put em on a disk and it s running.\n",
      "Expected: i know i met him.\n",
      "Generated: i m not sure what s going on. i m not sure what s going on.\n",
      "\n",
      "Input: chat: and i really wanted some ice cream too.\n",
      "Expected: i can run out and get it.\n",
      "Generated: i wanted ice cream. i wanted ice cream.\n",
      "\n",
      "Input: chat: no one.\n",
      "Expected: i heard voices.\n",
      "Generated: i don t know.\n",
      "\n",
      "Input: chat: might even incinerate the damn thing.\n",
      "Expected: i hope not.\n",
      "Generated: i m not sure if i m going to do that.\n",
      "\n",
      "Input: chat: straightening up indeed! are you sure you re not alone?\n",
      "Expected: i m always alone mrs. swicker you know that.\n",
      "Generated: i m not alone.\n",
      "\n",
      "Input: chat: by then it s too late! the only way to safely dispose of the nerve agent is in deep water! the chemicals will break down and dissipate!\n",
      "Expected: use your grenades! maybe we can blow it up underwater!\n",
      "Generated: it s too late!\n",
      "\n",
      "Input: chat: what do you mean?\n",
      "Expected: what i mean is you think if uh do you think if i came back... do you think you could forgive me?\n",
      "Generated: what do you mean?\n",
      "\n",
      "Input: chat: who ????\n",
      "Expected: listen i ve got to give you credit. your disguise is nearly perfect. i mean if you fooled me and i am nobody s fool\n",
      "Generated: i don t know. i don t know. i don t know.\n",
      "\n",
      "Input: chat: main street.\n",
      "Expected: doug the traffic light...\n",
      "Generated: what do you think?\n",
      "\n",
      "Input: chat: yes bob.\n",
      "Expected: did you say we saved ninety white people?\n",
      "Generated: i don t know.\n",
      "\n",
      "Input: chat: we got a bunch of u materials u coming up at auction. materials which disappeared from dr. lechter s cell drawings he made his u books u .\n",
      "Expected: yes?\n",
      "Generated: i m not sure what i ll do. i m not sure what i ll do.\n",
      "\n",
      "Input: chat: will the police do anything to her?\n",
      "Expected: it s too late for that. she can t drive the bus anymore. the school board saw to that right off.\n",
      "Generated: i don t know. i don t know.\n",
      "\n",
      "Input: chat: a bunch of lousy little spic car thieves.\n",
      "Expected: nothing in there except a new york street map.\n",
      "Generated: i don t know if you re a scrumptious man.\n",
      "\n",
      "Input: chat: do you own a video camera?\n",
      "Expected: no. fred hates them.\n",
      "Generated: i don t have a video camera.\n",
      "\n",
      "Input: chat: will you be spending more than a night?\n",
      "Expected: hard to say. i might have...some business here.\n",
      "Generated: i don t know if i d like to go out with you.\n",
      "\n",
      "Input: chat: oh. sorry.\n",
      "Expected: don t be. i m not. it s good money. side he s worth it.\n",
      "Generated: i m sorry. i m sorry.\n",
      "\n",
      "Input: chat: huh?\n",
      "Expected: i mean he s not serious he ll end up living off janey and jim you watch. it s just that you develop a sense when you get older if things are going to work out or if they won t and sometimes it s not worth the mess...\n",
      "Generated: i m not sure if you re going to be able to do that.\n",
      "\n",
      "Input: chat: what do you want? do i know you from somewhere?\n",
      "Expected: i want to know where my daughter is. her name is kristen or joanne. she s with you.\n",
      "Generated: i know you from somewhere.\n",
      "\n",
      "Input: chat: you boost her ?\n",
      "Expected: hell yeah. she s not my unicorn.\n",
      "Generated: i don t know.\n",
      "\n",
      "Input: chat: my name is manray goddamnit.\n",
      "Expected: kook and the gang it s manray. let s do the taping. you go back to your dressing room get dressed and blacken up.\n",
      "Generated: i m goddamnit.\n",
      "\n",
      "Input: chat: apparently not. don t you know where it is?\n",
      "Expected: no i m sorry.\n",
      "Generated: i don t know where it is.\n",
      "\n",
      "Input: chat: you are taking a chance that is not worth the risk.\n",
      "Expected: well we are one miracle short tonight. so just guard the stairs?\n",
      "Generated: i don t know if i have a chance. i don t know if i have a chance.\n",
      "\n",
      "Input: chat: i observe that you in one respect are a very fortunate man monsieur. i am moved to make one more suggestion why i do not know because it cannot possibly profit me but have you heard about signor ugarte and the letters of transit?\n",
      "Expected: yes something.\n",
      "Generated: i don t know. i don t know. i don t know.\n",
      "\n",
      "Input: chat: your mother thought he was a good man. he worked very hard. what i remember most was his manner was so... slight. it was easy to spend time in a room and not realize he d been there the whole time.\n",
      "Expected: was he morose or...? i mean...\n",
      "Generated: he was a good man. he s a good man.\n",
      "\n",
      "Input: chat: a little mixup in signals. let s go.\n",
      "Expected: go where?\n",
      "Generated: i m not sure what s going on. i don t know.\n",
      "\n",
      "Input: chat: sir this officer candidate requests permission to speak to you in private.\n",
      "Expected: i m busy mayo. it ll have to wait.\n",
      "Generated: i don t know if i d like to speak to you.\n",
      "\n",
      "Input: chat: i ve seen you out there in your old air boat thinking about what might have been old man van ryan hadn t fucked you out of it. what if i told you i had a way we could get a big piece of it back?\n",
      "Expected: we?\n",
      "Generated: i ve seen you out there in your old air boat thinking about what might have been old man van ryan.\n",
      "\n",
      "Input: chat: i had another dream last night. the dead man laying on top of me and i had nothing to eat. and the ghosts without skin stuck their fingers in me and said beloved in the dark and bitch in the light..\n",
      "Expected: don t say those things. you forget about those dreams..\n",
      "Generated: i had nothing to eat. i had nothing to eat.\n",
      "\n",
      "Input: chat: you look lovely.\n",
      "Expected: thank you. no i know you d prefer i m glad you u find u me so...\n",
      "Generated: i don t know what i mean.\n",
      "\n",
      "Input: chat: you re mad aren t you?\n",
      "Expected: no. yes of course i m mad because you love me i know that.\n",
      "Generated: i don t know.\n",
      "\n",
      "Input: chat: are you going to turn on me too? who helped you when you were strung out? who gave you money? who bailed you out of jail?\n",
      "Expected: i won t get away with it.\n",
      "Generated: i don t know. i don t know. i don t know. i don t know.\n",
      "\n",
      "Input: chat: so she hasn t changed?\n",
      "Expected: that i couldn t say. let me ask you something was she a little big boned in high school?\n",
      "Generated: i don t know what i m doing.\n",
      "\n",
      "Input: chat: laughs.\n",
      "Expected: it goes in that little hole.\n",
      "Generated: i m sorry.\n",
      "\n",
      "Input: chat: rock?\n",
      "Expected: see you re not that out of touch. you re good.\n",
      "Generated: i don t know what i mean.\n",
      "\n",
      "Input: chat: why how could i not worry? not knowin what s happenin to you or where you are? are you with that boy?\n",
      "Expected: if you mean sailor mama yes i am.\n",
      "Generated: i don t know what s happeningin to you?\n",
      "\n",
      "Input: chat: you can t work in an apartment where you owe three months rent.\n",
      "Expected: i ll take care of that.\n",
      "Generated: i don t want to work in a flat where you ll have three months rent.\n",
      "\n",
      "Input: chat: you asked if there was anything more you could help me with.\n",
      "Expected: when?\n",
      "Generated: i m sorry. i m sorry.\n",
      "\n",
      "Input: chat: did you see him?\n",
      "Expected: i ve never seen him. i don t believe there is a keeper of the files.\n",
      "Generated: he s a good guy. he s a good guy.\n",
      "\n",
      "Input: chat: you talk to me but you don t answer my questions. i guess i ll have to talk to lieutenant bedsoe.\n",
      "Expected: about what?\n",
      "Generated: i ll have to talk to lieutenant bedsoe.\n",
      "\n",
      "Input: chat: she left the party early. friends said she had a fight with her boyfriend and stormed out.\n",
      "Expected: what time was that?\n",
      "Generated: i m sorry.\n",
      "\n",
      "Input: chat: that s cool... hey look you re carl s brother right?\n",
      "Expected: that s right.\n",
      "Generated: i don t think you re a brother.\n",
      "\n",
      "Input: chat: wouldn t i. just watch me.\n",
      "Expected: chris chris !\n",
      "Generated: don t you?\n",
      "\n",
      "Input: chat: jor el be warned. this tone of yours approaches insolence.\n",
      "Expected: my friends i am not a rash impulsive person. i m not given to wild unsupported statements. we must evacuate this planet immediately!\n",
      "Generated: i don t know if i can t help but i can t help.\n",
      "\n",
      "Input: chat: yeah basement bingo.\n",
      "Expected: lemme see your ball.\n",
      "Generated: i don t know what i mean.\n",
      "\n",
      "Input: chat: to get my money.\n",
      "Expected: mookie you must think i m stupid or something. you re gonna run outta here and i won t see your black ass for another week.\n",
      "Generated: i don t know if i can t afford it. i don t know if i can afford it.\n",
      "\n",
      "Input: chat: who ll make the rules?\n",
      "Expected: we ll make em and we ll enforce em. all of us. together. we all get one vote. includin me.\n",
      "Generated: i m not sure. i m not sure. i m not sure what i ll do.\n",
      "\n",
      "Input: chat: no. can t say that i have richard. richard do you know why i m here?\n",
      "Expected: washington sent ya. i know that.\n",
      "Generated: i don t know why i m here.\n",
      "\n",
      "Input: chat: oh harry i m so proud of you making a real friend.\n",
      "Expected: is it okay if he spends the night?\n",
      "Generated: i m so proud of you making a real friend.\n",
      "\n",
      "Input: chat: don t know anything about it? i find it difficult to believe that i ve come millions of miles\n",
      "Expected: millions?\n",
      "Generated: i don t know what i m saying.\n",
      "\n",
      "Input: chat: my favorite memory was when we\n",
      "Expected: would you die already?!\n",
      "Generated: i m glad you re here.\n",
      "\n",
      "Input: chat: well... then may you rent me your gear for the day?\n",
      "Expected: i never rent my gear.\n",
      "Generated: i can t help but i can t help.\n",
      "\n",
      "Input: chat: actually yeah\n",
      "Expected: wouldn t happen to be the lincoln memorial would it?\n",
      "Generated: i don t know what i mean.\n",
      "\n",
      "Input: chat: i m just glad there aren t any hard feelings.\n",
      "Expected: oh none. none. i completely understand what was going on.\n",
      "Generated: i m sorry.\n",
      "\n",
      "Input: chat: sure mr. muss uh sid said i could have the job back.\n",
      "Expected: absolutely buzz i m glad he\n",
      "Generated: i don t know if i can get back to work.\n",
      "\n",
      "Input: chat: all this for laying traps on private land?\n",
      "Expected: you left a footprint at the sportsmen s camp. only pretty sight there ben cause the two men you didn t shoot and mutilate died of exposure.\n",
      "Generated: i don t know.\n",
      "\n",
      "Input: chat: yes.\n",
      "Expected: are your parents living?\n",
      "Generated: i don t know. i don t know.\n",
      "\n",
      "Input: chat: what are you doing here!?\n",
      "Expected: looking for hal. oh my god i heard someone kick the door open...you came back.\n",
      "Generated: what do you think?\n",
      "\n",
      "Input: chat: you rigged the illusion to fail right?\n",
      "Expected: yes.\n",
      "Generated: i don t know if i m going to fail.\n",
      "\n",
      "Input: chat: prophesy to us oh christ! who is it that struck thee?\n",
      "Expected: i don t know but god help him if he does it again!\n",
      "Generated: i m not a christian.\n",
      "\n",
      "Input: chat: why?\n",
      "Expected: he s wanted in japan. they want him first. then we can have him.\n",
      "Generated: i don t know why.\n",
      "\n",
      "Input: chat: well i d say who i was and do you remember me and ask if you d like to meet for a drink.\n",
      "Expected: if i remembered you. i came looking for you. i would ve said sure let s do it. but for all you knew i could show up with a swat team. why would you trust me?\n",
      "Generated: do you remember me?\n",
      "\n",
      "Input: chat: so what do you do?\n",
      "Expected: i m a dentist\n",
      "Generated: what do you do?\n",
      "\n",
      "Input: chat: i ve heard the arguments. like napoleon s quote there s one thing you can t do with a bayonet and that s sit on it.\n",
      "Expected: that s right sir. and don t forget in a few years a lot of other countries will have the bomb. what if they start something?\n",
      "Generated: i ve heard the arguments.\n",
      "\n",
      "Input: chat: well i ll be !!!\n",
      "Expected: what s the matter?\n",
      "Generated: i ll be there. i ll be there.\n",
      "\n",
      "Input: chat: you sound worried.\n",
      "Expected: me? if i m worried about anyone it s you. what happened to you sam? you were the brightest of us\n",
      "Generated: i m worried.\n",
      "\n",
      "Input: chat: david?\n",
      "Expected: exactly. now i ve been thinking\n",
      "Generated: i don t know. i don t know.\n",
      "\n",
      "Input: chat: sorry. i don t know anything about it.\n",
      "Expected: come on gimme a break.\n",
      "Generated: i don t know about it.\n",
      "\n",
      "Input: chat: but she still might call!\n",
      "Expected: are you coming or not?\n",
      "Generated: i don t know if she s gonna call her.\n",
      "\n",
      "Input: chat: the food the sky the certain uh sexual temptations you haven t become addicted have you cole? to that dying world\n",
      "Expected: no sir! i just want to do my part. to get us back on top...in charge of the planet. and i have the experience i know who the people are...\n",
      "Generated: i m not addicted. i m not addicted.\n",
      "\n",
      "Input: chat: you are in great danger. the dalai lama cannot fall into chinese hands. you must flee.\n",
      "Expected: how could i ever leave?\n",
      "Generated: i m not a chinese. i m not a chinese.\n",
      "\n",
      "Input: chat: oh yeah?\n",
      "Expected: let me heal you baby.\n",
      "Generated: oh yeah.\n",
      "\n",
      "Input: chat: i wouldn t have risked killing you to get you out. i was trying to protect you.\n",
      "Expected: from what?\n",
      "Generated: i didn t want to kill you. i didn t want to kill you.\n",
      "\n",
      "Input: chat: ready to deploy the warp nacelles.\n",
      "Expected: as they used to say... all systems are go.\n",
      "Generated: i don t know if i can t. i can t help but i can t help.\n",
      "\n",
      "Input: chat: did regan know a priest was coming over?\n",
      "Expected: no.\n",
      "Generated: i m a priest. i m not a priest.\n",
      "\n",
      "Input: chat: tom! they s a whole lot i don t understan but goin away ain t gonna ease us. they was the time when we was on the lan . they was a bound ry to us then. ol folks died off an little fellas come an we was always one thing we was the fambly kinda whole an clear. but now we ain t clear no more. they ain t nothin keeps us clear. al he s a hankerin an a jibbitin to go off on his own. an uncle john is just a draggin along. pa s lost his place he ain t the head no more. we re crackin up tom. they ain t no fambly now. rosasharn she gonna have her baby but it ain t gonna have no fambly. i been tryin to keep her goin but winfiel what s he gonna be this a way? growin up wild an ruthie too like animals. got nothin to trus . don t go tom. stay an help. help me.\n",
      "Expected: okay ma. i shouldn t though. i know i shouldn t. but okay.\n",
      "Generated: don t go tom. don t go tom. don t go tom.\n",
      "\n",
      "Input: chat: and father i ve never been guilty of any institutional infractions have i?\n",
      "Expected: you certainly have not 655321. you ve been very helpful and you ve shown a genuine desire to reform.\n",
      "Generated: i ve never been guilty of any institution infractions.\n",
      "\n",
      "Input: chat: just...try and go back to sleep.\n",
      "Expected: i get dreams. i don t like em.\n",
      "Generated: i don t want to go back to sleep.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your test data (replace this with your own test set)\n",
    "# Example format: list of (input, expected_output)\n",
    "# For Cornell, it might be something like:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# from test_dataset keep only 'input' and 'output' columns\n",
    "test_data = [(item['input'], item['output']) for item in test_dataset]\n",
    "test_data = test_data[:100]  # Limit to first 100 for testing\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate(model, tokenizer, test_data):\n",
    "    results = []\n",
    "    for source, expected in tqdm(test_data):\n",
    "        input_ids = tokenizer.encode(source, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_length=50,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        results.append({\n",
    "            \"Input\": source,\n",
    "            \"Expected\": expected,\n",
    "            \"Generated\": generated\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(model, tokenizer, test_data)\n",
    "\n",
    "# Print results\n",
    "for r in results:\n",
    "    print(f\"Input: {r['Input']}\")\n",
    "    print(f\"Expected: {r['Expected']}\")\n",
    "    print(f\"Generated: {r['Generated']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14e4ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0112\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "scores = []\n",
    "for r in results:\n",
    "    ref = [r[\"Expected\"].split()]\n",
    "    hyp = r[\"Generated\"].split()\n",
    "    score = sentence_bleu(ref, hyp, smoothing_function=smoothie)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Average BLEU Score: {sum(scores)/len(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f08205",
   "metadata": {},
   "source": [
    "Using DailyDialog+T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7b71977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daily_dialog\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "904887f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9?.!,¿' ]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_function(example):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    dialogue = example[\"dialog\"]\n",
    "    for i in range(len(dialogue) - 1):\n",
    "        input_text = \"chat: \" + clean_text(dialogue[i])\n",
    "        target_text = clean_text(dialogue[i + 1])\n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    return {\"input_texts\": inputs, \"target_texts\": targets}\n",
    "\n",
    "processed_dataset = dataset[\"train\"].map(preprocess_function, remove_columns=dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b90e3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    model_inputs = tokenizer(example[\"input_texts\"], max_length=40, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(example[\"target_texts\"], max_length=40, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = processed_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc8d2358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_texts', 'target_texts', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c9f5e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many sentences are there per row\n",
    "# make a new dataset with only 1 sentence per row\n",
    "def flatten_dataset(dataset):\n",
    "    flattened_data = []\n",
    "    for i in range(len(dataset)):\n",
    "        input_texts = dataset[i][\"input_texts\"]\n",
    "        target_texts = dataset[i][\"target_texts\"]\n",
    "        input_ids = dataset[i][\"input_ids\"]\n",
    "        labels = dataset[i][\"labels\"]\n",
    "        attention_mask = dataset[i][\"attention_mask\"]\n",
    "\n",
    "        for j in range(len(input_texts)):\n",
    "            flattened_data.append({\n",
    "                \"input_text\": input_texts[j],\n",
    "                \"target_text\": target_texts[j],\n",
    "                \"input_ids\": input_ids[j],\n",
    "                \"attention_mask\": attention_mask[j],\n",
    "                \"labels\": labels[j]\n",
    "\n",
    "            })\n",
    "    return flattened_data\n",
    "\n",
    "flattened_data = flatten_dataset(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1724fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "flattened_dataset = pd.DataFrame(flattened_data)\n",
    "hf_dataset = Dataset.from_pandas(flattened_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05a34bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 68446\n",
      "Validation dataset size: 7606\n"
     ]
    }
   ],
   "source": [
    "train_test = hf_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b3aacc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat: good.let ' s go now .\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0]['input_texts'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b312de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat: say , jim , how about going for a few beers after dinner ?\n",
      "you know that is tempting but is really not good for our fitness .\n",
      "[3582, 10, 497, 3, 6, 3, 354, 603, 3, 6, 149, 81, 352, 21, 3, 9, 360, 36, 277, 227, 2634, 3, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[25, 214, 24, 19, 24873, 68, 19, 310, 59, 207, 21, 69, 4639, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "chat: say, jim, how about going for a few beers after dinner?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "you know that is tempting but is really not good for our fitness.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Input length: 40\n",
      "Output length: 40\n",
      "Input length: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Show a sample from the dataset\n",
    "print(tokenized_dataset[0][\"input_texts\"][0])  # Input text\n",
    "print(tokenized_dataset[0][\"target_texts\"][0])  # Target text\n",
    "print(tokenized_dataset[0][\"input_ids\"][0])  # Tokenized input IDs\n",
    "print(tokenized_dataset[0][\"labels\"][0])  # Tokenized target IDs\n",
    "print(tokenizer.decode(tokenized_dataset[0][\"input_ids\"][0]))  # Decoded input text\n",
    "print(tokenizer.decode(tokenized_dataset[0][\"labels\"][0]))  # Decoded target text\n",
    "\n",
    "print(\"Input length:\", len(tokenized_dataset[0][\"input_ids\"][0]))  # Length of input IDs\n",
    "print(\"Output length:\", len(tokenized_dataset[0][\"labels\"][0]))  # Length of target IDs\n",
    "print(\"Input length:\", tokenized_dataset[0][\"attention_mask\"][0])  # Length of input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19770c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 06:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('t5-dailydialog-model\\\\tokenizer_config.json',\n",
       " 't5-dailydialog-model\\\\special_tokens_map.json',\n",
       " 't5-dailydialog-model\\\\spiece.model',\n",
       " 't5-dailydialog-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(1000))  # Select first 1000 samples\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(100))  # Select first 100 samples\n",
    "\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "eval_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-dailydialog\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # For demonstration; use validation set in practice\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"t5-dailydialog-model\")\n",
    "tokenizer.save_pretrained(\"t5-dailydialog-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it.\n",
      "we're not able to make our own decision on the subject of a conversation between people who have a business, we will. We can get a lot of advice for other individuals.\n",
      "en anglais?\n",
      "can you picture your holiday on your beach beach?\n",
      "Input: chat: have you tried an outlet?\n",
      "Expected: why didn't i think of that?\n",
      "Generated: i have a problem with a.\n",
      "\n",
      "Input: chat: suit wrote me a letter.\n",
      "Expected: what did she say?\n",
      "Generated: \n",
      "\n",
      "Input: chat: what do you think of this one?\n",
      "Expected: eh, so so.\n",
      "Generated: i'm not sure if you're going to be able to.\n",
      "\n",
      "Input: chat: oh, i thought i could make a right turn on red here.\n",
      "Expected: no, sir. the sign says no turn on red.\n",
      "Generated: i thought i could make a right turn on red.\n",
      "\n",
      "Input: chat: sorry. i feel like sitting out the next dance.\n",
      "Expected: ok. let's get something to drink.\n",
      "Generated: i'm not sure. but i'm not sure.\n",
      "\n",
      "Input: chat: i don't know what to suggest. there're so many attractions, and they all sound interesting. one exciting program might be the ten thousand people cycling\n",
      "Expected: i'd like to take the one that goes to the wild animal park.\n",
      "Generated: i'm not sure. i'm not sure.\n",
      "\n",
      "Input: chat: excellent. would you like anything else?\n",
      "Expected: no, thank you.\n",
      "Generated: i'm not sure.\n",
      "\n",
      "Input: chat: what should i do about it?\n",
      "Expected: you may feel much better if you chew some gum or peanuts.\n",
      "Generated: i'm not sure if i can get a good deal of money.\n",
      "\n",
      "Input: chat: chinese, maths, english, chemistry, physics, biology and so on.\n",
      "Expected: what do you think about the teachers?\n",
      "Generated: \n",
      "\n",
      "Input: chat: yes. are you?\n",
      "Expected: yes. we must have the same exam tomorrow!\n",
      "Generated: i'm not sure.\n",
      "\n",
      "Input: chat: i got it. do you pay for the programs?\n",
      "Expected: yes, 50 yuan per month.\n",
      "Generated: i have a sex.\n",
      "\n",
      "Input: chat: i believe most of them think of it as good public relations. they can promote their product and make their company betterknown.\n",
      "Expected: oh, i see. so factory tours are good for companies as well as visitors.\n",
      "Generated: \n",
      "\n",
      "Input: chat: yes.\n",
      "Expected: please follow the nurse. she ll get you ready.\n",
      "Generated: \n",
      "\n",
      "Input: chat: i like to travel light so i just have this one.\n",
      "Expected: if that's your only piece of luggage, it is small enough to carry on with you. would you like to do that so you don't have to wait in luggage\n",
      "Generated: i like to travel light. i like to travel light.\n",
      "\n",
      "Input: chat: i hear you have just shown your hand first to lin ping.\n",
      "Expected: yes.\n",
      "Generated: i'm sure you will be able to re-open the door.\n",
      "\n",
      "Input: chat: well, we'll certainly miss you in boston.\n",
      "Expected: i'm going to miss you folks, too.\n",
      "Generated: \n",
      "\n",
      "Input: chat: how is your computer skill?\n",
      "Expected: my familiarity to computer is great, and i use computer frequently in my daily life and work.\n",
      "Generated: i have a computer.\n",
      "\n",
      "Input: chat: hey mel! are you up for some tennis today?\n",
      "Expected: sorry, i can t! i have to go to work, pick up jake and maddie from school, and make them an afternoon snack, then\n",
      "Generated: i'm a tennis fan, i'm a sailor, i'm a sailor, i'm a sailor.\n",
      "\n",
      "Input: chat: i'd rather get one with separated speakers.they give a clear sound.\n",
      "Expected: yes, but we shouldn't get any thing too big.remember it has to fit in with our living room furniture.\n",
      "Generated: i'd rather get one with separated speakers.they give a clear sound.\n",
      "\n",
      "Input: chat: you re kidding. it can t be true.\n",
      "Expected: believe it or not. everybody is talking about it in the company.\n",
      "Generated: i m not sure.\n",
      "\n",
      "Input: chat: what instructions?\n",
      "Expected: we will give you a detailed instruction manual if you buy the table.\n",
      "Generated: i have a problem with a tv.\n",
      "\n",
      "Input: chat: i want to have a foot massage and a haircut.\n",
      "Expected: a foot massage sounds like a great idea. they are very relaxing. i d also like to have mudpack on my face. it s supposed to\n",
      "Generated: i want to have a foot massage and a haircut.\n",
      "\n",
      "Input: chat: have you updated your cv?\n",
      "Expected: nope.\n",
      "Generated: i'm not sure if you have a problem.\n",
      "\n",
      "Input: chat: yes, our delivery fee could be waived, if you make an order of 50 units or more. we would have to insist on the annual contract, however\n",
      "Expected: that would be fine. we could accept a lower rebate.\n",
      "Generated: i would like to say, i would like to say, i would like to say, i would like to say, i would like to say, i would like to say,\n",
      "\n",
      "Input: chat: are bananas on sale today?\n",
      "Expected: here you go. look in this flyer, or check with the produce person.\n",
      "Generated: \n",
      "\n",
      "Input: chat: abc rentacar. may i help you?\n",
      "Expected: i would like to rent a car. what kind of car do you have?\n",
      "Generated: i'm not sure if you have any questions.\n",
      "\n",
      "Input: chat: that s too bad. how long was the flight?\n",
      "Expected: it was 10 hours.\n",
      "Generated: i m not sure. i m sorry.\n",
      "\n",
      "Input: chat: cool!\n",
      "Expected: ok, each player gets 16 pieces. you can be the white ones and i ll play with the black pieces. now in the front, you set up the\n",
      "Generated: i'm not sure if i'm going to be able to re-read it.\n",
      "\n",
      "Input: chat: i know that i should learn how to save more, but i hope that we can spend a little on ourselves sometimes.\n",
      "Expected: i m sorry. i guess i m too extreme when it comes putting money in the bank.\n",
      "Generated: i'm not sure if i can save more. but i think i'll save more.\n",
      "\n",
      "Input: chat: it s that way, on the second floor.\n",
      "Expected: thanks.\n",
      "Generated: i m a s a s a s a s a s a s a s a s a s a s a s\n",
      "\n",
      "Input: chat: jacob, you have the luxury of having a haircut that rarely needs styling. i don t. i have to set aside about an hour and\n",
      "Expected: true, i hardly ever see you without your hair done and your makeup on, even when you show up to class in sweatpants. tell me, how long\n",
      "Generated: i have to set aside about an hour and a half.\n",
      "\n",
      "Input: chat: good morning mike!\n",
      "Expected: morning sally! what's up? you seem in a hurry!\n",
      "Generated: i'm going to be back in the morning.\n",
      "\n",
      "Input: chat: all right. if we win a lot of money, we shall travel around the world and we shall stay at the best hotels. then we will return home and\n",
      "Expected: but if we spend all that money we shall be poor again. what will we do then?\n",
      "Generated: i'm not sure. i'm going to be a big winner.\n",
      "\n",
      "Input: chat: how about the new bar across road?\n",
      "Expected: sounds good. the food there is fantastic too.\n",
      "Generated: i'm not sure if i'll be able to reopen.\n",
      "\n",
      "Input: chat: oh? what's wrong with his girlfriend?\n",
      "Expected: she came over last weekend. she is a nice girl but there's something about her voice that really creeps me out. i tried to smile and be polite\n",
      "Generated: i'm not sure if i'm going to be a good guy.\n",
      "\n",
      "Input: chat: you want to be a politician? yuck.\n",
      "Expected: that's not like you to generalize.\n",
      "Generated: i'm a politician.\n",
      "\n",
      "Input: chat: if you gave him another 30 days, do you think he could figure it out by then?\n",
      "Expected: to be honest, he has no authority. he's a hardworker, but no one listens to him.\n",
      "Generated: i'm not sure if i could.\n",
      "\n",
      "Input: chat: yes, arm, i'm 23. i've been working abroad, i'm um...\n",
      "Expected: where exactly have you been working, please?\n",
      "Generated: i'm 23.\n",
      "\n",
      "Input: chat: she is very nice and openminded.\n",
      "Expected: much better than the last one, huh?\n",
      "Generated: \n",
      "\n",
      "Input: chat: that's exactly what the visa officers want to hear. do you have enough money for tuition and room and board?\n",
      "Expected: i've received a full scholarship, so i won't need any other money to live off while i'm studying.\n",
      "Generated: i'm not sure if i'm going to be able to get a visa.\n",
      "\n",
      "Input: chat: hey, it is! a lot of people like that movie!\n",
      "Expected: i'm sorry, but it's a bad movie. the plot is stupid, and the script is poorly written.\n",
      "Generated: i'm not sure if i'll be able to.\n",
      "\n",
      "Input: chat: it s exactly twelve minutes past seven.\n",
      "Expected: when will you lecture begin?\n",
      "Generated: i m not sure if i can t.\n",
      "\n",
      "Input: chat: whom would you like to talk to?\n",
      "Expected: richard zhang.\n",
      "Generated: \n",
      "\n",
      "Input: chat: i think housing is a big problem. there are thousands of homeless people on the streets.\n",
      "Expected: how would you solve the problem?\n",
      "Generated: i think housing is a big problem.\n",
      "\n",
      "Input: chat: can i see all of my test results at once?\n",
      "Expected: yes, you can see every test for the past five years. you can compare them.\n",
      "Generated: i can't see my test results.\n",
      "\n",
      "Input: chat: there's an ethernet cable in your room. just plug it into the back of your laptop.\n",
      "Expected: uhoh! i can't use ethernet. my computer is wireless only.\n",
      "Generated: i'm not sure if you're going to have to.\n",
      "\n",
      "Input: chat: let me also have three pounds of chicken breasts.\n",
      "Expected: that's going to be 4.05 a pound.\n",
      "Generated: i'm a chicken.\n",
      "\n",
      "Input: chat: about like this?\n",
      "Expected: no, not that much. and yes, that fine.\n",
      "Generated: i'm not sure if i'm going to be able to.\n",
      "\n",
      "Input: chat: i ll also set some cups and saucers for some coffee after dinner.\n",
      "Expected: honey? have you seen our soup bowls?\n",
      "Generated: i ll set some cups and saucers for some coffee.\n",
      "\n",
      "Input: chat: how much is the voucher worth?\n",
      "Expected: it is worth 100 yuan and your bill comes to 230 yuan. the difference is 130, please.\n",
      "Generated: i have a good time.\n",
      "\n",
      "Input: chat: which bus should i take from phs to sons?\n",
      "Expected: can you tell me which sons you want to go to?\n",
      "Generated: i'm a sailor, i'm a sailor, i'm a sailor.\n",
      "\n",
      "Input: chat: i'm sorry, but this was on sale, so i can only give you store credit.\n",
      "Expected: fine. here's my receipt. i'd better be able to use this anywhere in the store.\n",
      "Generated: i'm sorry, but this was on sale. i can only give you store credit.\n",
      "\n",
      "Input: chat: by the way, what should i do with the key when i go out?\n",
      "Expected: please drop it at the front desk when you leave tie hotel.\n",
      "Generated: i'm not sure if the key is right.\n",
      "\n",
      "Input: chat: he is really great. he has sold millions of records worldwide. he is especially known for his rocking performance at the football world cup songthe cup of world\n",
      "Expected: i heard one observer even said'it was a song sung in spanish for international football, and he made it so popular that even the americans loved\n",
      "Generated: i'm sure. i'm going to be a big fan.\n",
      "\n",
      "Input: chat: today?\n",
      "Expected: yes.\n",
      "Generated: i'm not sure if you have any questions.\n",
      "\n",
      "Input: chat: i've never taken this bus, but i think it's the right one.\n",
      "Expected: this place doesn't look like altadena.\n",
      "Generated: i'm not sure if i'll take this bus.\n",
      "\n",
      "Input: chat: what do you do in winter?\n",
      "Expected: well, i play sports indoors quite often. if i m feeling lazy, i just watch a film at home. i prefer summer to\n",
      "Generated: i have a good time.\n",
      "\n",
      "Input: chat: of course not, i appreciate that.\n",
      "Expected: well, it has nothing to do with virus. the problem is your attachment is a bit larger. it has exceeded the email capacity.\n",
      "Generated: i am not sure, but i am sure.\n",
      "\n",
      "Input: chat: please fill in this transfer form and enter your password, here. then the transaction should be completed.\n",
      "Expected: ok... there we go. thanks very much.\n",
      "Generated: i'm not sure. i'm not sure.\n",
      "\n",
      "Input: chat: there s an interesting interview with a top fashion designer about the latest fashions. i enjoyed reading her thought. the which? section is very interesting this month\n",
      "Expected: i like to take the tests that they print in this magazine.\n",
      "Generated: \n",
      "\n",
      "Input: chat: what are our areas for growth? what sectors do you see the most potential in? if we are going to pull our sales numbers up and develop the brand, we\n",
      "Expected: it's not just spreading out to new markets that we have to address. i think we'd better first pay attention to developing our brand in the markets we already have.we\n",
      "Generated: i'm going to be able to build a brand. i'm going to be able to build a brand.\n",
      "\n",
      "Input: chat: hi, i'm calling from nika corporation. we would like to hold a business lunch at the restaurant.\n",
      "Expected: oh, certainly. i'm linda, the manager. i can help you with that. how many will there be in your party\n",
      "Generated: i'm a nika corporation.\n",
      "\n",
      "Input: chat: what do you think of our new teacher?\n",
      "Expected: professor wood? i think he's a brilliant scientist.\n",
      "Generated: \n",
      "\n",
      "Input: chat: is it safe to job hunt on the internet?\n",
      "Expected: if you log in some formal websites, it must be very safe.\n",
      "Generated: i have a job i have a job.\n",
      "\n",
      "Input: chat: where did you learn it?\n",
      "Expected: at school. and i'm still learning it now.\n",
      "Generated: \n",
      "\n",
      "Input: chat: what's the news on our website? how effective do you think it is from a marketing standpoint?\n",
      "Expected: we've been able to survey and track some of the information of our website users through some cookie technology, and it's surprising to see the results.\n",
      "Generated: i'm not sure if you're going to be able to get a job.\n",
      "\n",
      "Input: chat: well, he should pay you back. it s only fair.\n",
      "Expected: yes, but is got angry with him too quickly. he probably thought that i mistrusted him. i shouldn t have got angry.\n",
      "Generated: i m not sure.\n",
      "\n",
      "Input: chat: everybody who walks past me sees it. besides, it's a good sunbathing suit.\n",
      "Expected: all right. suit yourself. i am going swimming.\n",
      "Generated: i'm not sure.\n",
      "\n",
      "Input: chat: it is my honor to have this chance for this interview. well, my major is commercial english. you know, and i am not only familiar with useful english\n",
      "Expected: what is your greatest weakness?\n",
      "Generated: \n",
      "\n",
      "Input: chat: what's that?\n",
      "Expected: you come back here with me tomorrow, and it's my treat!\n",
      "Generated: i'm not sure if i'm going to be able to do that.\n",
      "\n",
      "Input: chat: it's not that difficult. if you want to watch a game with me, i can explain the rules and the tactics while we're watching.\n",
      "Expected: thanks, that'd be nice. when's the next ice hockey game on?\n",
      "Generated: i'm not sure. i'm going to play.\n",
      "\n",
      "Input: chat: yes, i'm going to take a holiday tomorrow.\n",
      "Expected: how nice! i can see you are busying packing. i'm sorry to interrupt you.\n",
      "Generated: \n",
      "\n",
      "Input: chat: just since today.\n",
      "Expected: you can still drive, without problems.\n",
      "Generated: i'm not sure if i'll be able to.\n",
      "\n",
      "Input: chat: telco mobile, how can i help you?\n",
      "Expected: yes, i d like to activate my voice mail service please.\n",
      "Generated: i can't help you.\n",
      "\n",
      "Input: chat: do you get along well with each other?\n",
      "Expected: yes, we are very close. he is 12 years old and very smart. he always makes us laugh a lot.\n",
      "Generated: i'm not sure if you can.\n",
      "\n",
      "Input: chat: i'll see you in sterlet, then.\n",
      "Expected: where did you say?\n",
      "Generated: i'll see you in sterlet.\n",
      "\n",
      "Input: chat: wow! what happened to you? you look sad.\n",
      "Expected: i just lost my job. my boss just told me.\n",
      "Generated: i'm sorry.\n",
      "\n",
      "Input: chat: ok, whatever. see, i'm putting it in the garbage can.\n",
      "Expected: hold on, that's an item. you should put that in the recycle bin next to the trashcan.\n",
      "Generated: i'm putting it in the garbage can.\n",
      "\n",
      "Input: chat: why not? i love animals and i love nature.\n",
      "Expected: you can see many cows and horses there. lf you are brave enough, you can try horseback riding. it is fun.\n",
      "Generated: i love animals, i love nature, i love animals.\n",
      "\n",
      "Input: chat: o. k. do you have your receipt?\n",
      "Expected: yes, here you are.\n",
      "Generated: i have a receipt.\n",
      "\n",
      "Input: chat: may i try this on?\n",
      "Expected: sure. the color is perfect for you.\n",
      "Generated: \n",
      "\n",
      "Input: chat: what's net love?\n",
      "Expected: two people chat on the net and step by step fall into love! it's net love.\n",
      "Generated: i'm not sure if you're going to be a fan.\n",
      "\n",
      "Input: chat: sure. it s not very heavy.\n",
      "Expected: that s 850 grams. the coasts depends on how you would like to send it.\n",
      "Generated: i m not a big. i m not a big.\n",
      "\n",
      "Input: chat: not always. but they like a good meal. lots of tourists go to rome just for the food, you know.\n",
      "Expected: really? how much does it cost to fly to rome?\n",
      "Generated: i'm not sure if i'll be able to eat.\n",
      "\n",
      "Input: chat: can you do the vault?\n",
      "Expected: i do back flips at the pool all the time.\n",
      "Generated: i'm not sure if i can.\n",
      "\n",
      "Input: chat: yeah. you know what dave just said?\n",
      "Expected: what?\n",
      "Generated: i'm not sure.\n",
      "\n",
      "Input: chat: thanks a lot, bye bye.\n",
      "Expected: you are welcome, bye.\n",
      "Generated: i'm sorry.\n",
      "\n",
      "Input: chat: no, i never do that.\n",
      "Expected: why not?\n",
      "Generated: i don't know.\n",
      "\n",
      "Input: chat: no, sir. the bridges are fine.but i've heard rumors that the revolutionaries are coming down from the mountains.there has been fighting\n",
      "Expected: are you serious? fighting? but i didn't hear anything on the news.\n",
      "Generated: \n",
      "\n",
      "Input: chat: i just think it will be a little difficult for her to be away from home for so long right off the bat.\n",
      "Expected: you have a good point. she hasn't really ever been away from home for longer than a few hours.\n",
      "Generated: \n",
      "\n",
      "Input: chat: confirm the question if you don't catch it clearly.\n",
      "Expected: try to avoid the possible duplicate answer if you can.\n",
      "Generated: i'm not sure if you're a good person.\n",
      "\n",
      "Input: chat: may's birthday is coming. shall we buy her a birthday present or let her choose one for herself?\n",
      "Expected: i think a surprise party may be better. but i forget when her birthday is.\n",
      "Generated: \n",
      "\n",
      "Input: chat: when you use the internet, be careful not to give out your email address very often. if you do, you might get a lot of spamunwanted\n",
      "Expected: that s good advice. i should also be careful about giving out confidential information about myself, such as my password and credit car number.\n",
      "Generated: i have a lot of spamunwanted.\n",
      "\n",
      "Input: chat: en?\n",
      "Expected: it was a good company to work for, i enjoyed my time with them.\n",
      "Generated: \n",
      "\n",
      "Input: chat: let me take your temperature with a thermometer.\n",
      "Expected: ok.\n",
      "Generated: i'm going to take your temperature with a thermometer.\n",
      "\n",
      "Input: chat: don't cut into them until you can feel that they're soft on the outside.\n",
      "Expected: what country produces them?\n",
      "Generated: i'm not sure if they're soft.\n",
      "\n",
      "Input: chat: ok. goodbye.\n",
      "Expected: see you later.\n",
      "Generated: i'm sorry.\n",
      "\n",
      "Input: chat: where is our waitress anyway?\n",
      "Expected: yes, the service hasn't been the best. is that our waitress over there?\n",
      "Generated: i'm not sure if we have a problem with the waitress.\n",
      "\n",
      "Input: chat: but there's no hot water after 10 at night. sometimes i like to have a hot shower before bed. especially in the winter. it feels good\n",
      "Expected: that's true. but i love the living room it's huge and filled with sunshine.\n",
      "Generated: i'm not sure if i'll have to wait.\n",
      "\n",
      "Input: chat: no problem.\n",
      "Expected: after watching the movie. are you crying?\n",
      "Generated: i have a problem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat(prompt):\n",
    "    input_ids = tokenizer(\"chat: \" + prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=50, do_sample=True, top_k=50, top_p=0.95)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(chat(\"Hello! How are you?\"))\n",
    "print(chat(\"Tell me a joke.\"))\n",
    "print(chat(\"What is your name?\"))\n",
    "print(chat(\"What is the weather like today?\"))\n",
    "\n",
    "# Evaluate the model on the val set\n",
    "def evaluate(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for example in dataset:\n",
    "        input_ids = example[\"input_ids\"].unsqueeze(0)\n",
    "        attention_mask = example[\"attention_mask\"].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
    "\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        expected_text = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
    "        results.append({\n",
    "            \"Input\": tokenizer.decode(input_ids[0], skip_special_tokens=True),\n",
    "            \"Expected\": expected_text,\n",
    "            \"Generated\": generated_text\n",
    "        })\n",
    "    return results\n",
    "results = evaluate(model, tokenizer, eval_dataset[:10])\n",
    "for r in results:\n",
    "    print(f\"Input: {r['Input']}\")\n",
    "    print(f\"Expected: {r['Expected']}\")\n",
    "    print(f\"Generated: {r['Generated']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885eef2",
   "metadata": {},
   "source": [
    "Using BlenderBot by Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c57f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e771c839c9483b99b0c6251bea7b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\japne\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\japne\\.cache\\huggingface\\hub\\models--facebook--blenderbot-400M-distill. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8090ed6248ad412394ee83718fc66529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab1fff64ef3431ab361219785266d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/62.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c685478e5a42a68ce809d5ca8d1afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb19b96a0d7a4ffa81b0de41bc42b380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8954728162849faa93d50191e4c73ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/310k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ecd301d94c4c85900b7079495e64cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f62f223a9b43e297768c449ed0a4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/730M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c309c2c35b4c56adf234df09b65c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c64b884bf6c4ae786d4956c00e6c0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/730M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\n",
      " Sure, one of my favorite facts is that the word \"fact\" was first recorded in the 10th century in a Latin manuscript from the Southern Italy town of Gaeta in Lazio.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def chat(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    reply_ids = model.generate(**inputs)\n",
    "    reply = tokenizer.decode(reply_ids[0], skip_special_tokens=True)\n",
    "    return reply\n",
    "\n",
    "print(chat(\"Hey! How are you today?\"))\n",
    "print(chat(\"Can you tell me a fun fact?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d16b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't really have a favorite movie, but I do like action movies. What about you?\n",
      " What do you call a deer with no teeth?  A duck.  A pig.\n",
      " It is a little chilly, but not too bad.  How about where you are?\n",
      " Sure, I was in a car accident and had to get stitches in my head.\n",
      " My favorite color is blue.  What is yours?  Do you have a favorite color as well?\n"
     ]
    }
   ],
   "source": [
    "# Sample conversation\n",
    "print(chat(\"What is your favorite movie?\"))\n",
    "print(chat(\"Tell me a joke.\"))\n",
    "print(chat(\"What is the weather like today?\"))\n",
    "print(chat(\"Can you tell me a story?\"))\n",
    "print(chat(\"What is your favorite color?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29641c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\japne\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 60, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./blenderbot_400M\"\n",
    "\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbb5f5",
   "metadata": {},
   "source": [
    "Applying seq2seq on medical dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b534337d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Legacy ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Speciality</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>Publish At</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01JF4TEQHFJ5MEKH1QNM9PRT92</td>\n",
       "      <td>897840</td>\n",
       "      <td>What are effective therapies for metastatic br...</td>\n",
       "      <td>Dr. Pawar Satyajit Jalinder</td>\n",
       "      <td>Medical oncology</td>\n",
       "      <td>Metastatic breast cancer occurs when cancer sp...</td>\n",
       "      <td>icliniq</td>\n",
       "      <td>2024-12-15T15:00:05</td>\n",
       "      <td>Hello doctor,\\nMy mother was diagnosed with st...</td>\n",
       "      <td>Hello,\\nWelcome to icliniq.com.\\nI can underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01JF4Q0WT937DQDC2MGR40MTDN</td>\n",
       "      <td>3595800</td>\n",
       "      <td>How does HIV spread?</td>\n",
       "      <td>Dr. Basti Bharatesh Devendra</td>\n",
       "      <td>Dermatology</td>\n",
       "      <td>HIV spreads by certain body fluids from an inf...</td>\n",
       "      <td>icliniq</td>\n",
       "      <td>2024-12-15T14:00:04</td>\n",
       "      <td>Hello doctor,Last night I went for dinner and ...</td>\n",
       "      <td>Hello,\\nWelcome to icliniq.com.\\nI read your q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01JF4KK3NKG5ZRRZ3G0QFXFAB7</td>\n",
       "      <td>3099775</td>\n",
       "      <td>Can recurrent hoarseness without GERD indicate...</td>\n",
       "      <td>Dr. Akshay. B. K.</td>\n",
       "      <td>Otolaryngology (E.N.T)</td>\n",
       "      <td>Recurrent hoarseness may result from vocal str...</td>\n",
       "      <td>icliniq</td>\n",
       "      <td>2024-12-15T13:00:04</td>\n",
       "      <td>Hi doctor,I am a 59-year-old male and a nonsmo...</td>\n",
       "      <td>Hi,\\nWelcome to icliniq.com.\\nI have read your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01JF4G52SP8WMFBKZ5V075R066</td>\n",
       "      <td>65337</td>\n",
       "      <td>Is long-term Pantocid-IT use safe?</td>\n",
       "      <td>Dr. Kunal Das</td>\n",
       "      <td>Medical Gastroenterology</td>\n",
       "      <td>Long-term use of Pantocid-IT may cause nutrien...</td>\n",
       "      <td>icliniq</td>\n",
       "      <td>2024-12-15T12:00:04</td>\n",
       "      <td>Hi doctor,\\nI am a 35-year-old male. My height...</td>\n",
       "      <td>Hi,\\nWelcome to icliniq.com.\\nI have read your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01JF4CQ76T1P7713XQ099A432S</td>\n",
       "      <td>3876269</td>\n",
       "      <td>Can type 2 diabetes resolve after delivery?</td>\n",
       "      <td>Dr. Nitesh Goyal</td>\n",
       "      <td>Pulmonology (Asthma Doctors)</td>\n",
       "      <td>After giving birth, a person with type 1 diabe...</td>\n",
       "      <td>icliniq</td>\n",
       "      <td>2024-12-15T11:00:04</td>\n",
       "      <td>Hi doctor,My sister delivered a baby one month...</td>\n",
       "      <td>Hi,\\nWelcome to icliniq.com.\\nI have gone thro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ID  Legacy ID  \\\n",
       "0  01JF4TEQHFJ5MEKH1QNM9PRT92     897840   \n",
       "1  01JF4Q0WT937DQDC2MGR40MTDN    3595800   \n",
       "2  01JF4KK3NKG5ZRRZ3G0QFXFAB7    3099775   \n",
       "3  01JF4G52SP8WMFBKZ5V075R066      65337   \n",
       "4  01JF4CQ76T1P7713XQ099A432S    3876269   \n",
       "\n",
       "                                               Title  \\\n",
       "0  What are effective therapies for metastatic br...   \n",
       "1                               How does HIV spread?   \n",
       "2  Can recurrent hoarseness without GERD indicate...   \n",
       "3                 Is long-term Pantocid-IT use safe?   \n",
       "4        Can type 2 diabetes resolve after delivery?   \n",
       "\n",
       "                    Author Name                    Speciality  \\\n",
       "0   Dr. Pawar Satyajit Jalinder              Medical oncology   \n",
       "1  Dr. Basti Bharatesh Devendra                   Dermatology   \n",
       "2             Dr. Akshay. B. K.        Otolaryngology (E.N.T)   \n",
       "3                 Dr. Kunal Das      Medical Gastroenterology   \n",
       "4              Dr. Nitesh Goyal  Pulmonology (Asthma Doctors)   \n",
       "\n",
       "                                            Abstract Affiliation  \\\n",
       "0  Metastatic breast cancer occurs when cancer sp...     icliniq   \n",
       "1  HIV spreads by certain body fluids from an inf...     icliniq   \n",
       "2  Recurrent hoarseness may result from vocal str...     icliniq   \n",
       "3  Long-term use of Pantocid-IT may cause nutrien...     icliniq   \n",
       "4  After giving birth, a person with type 1 diabe...     icliniq   \n",
       "\n",
       "            Publish At                                           Question  \\\n",
       "0  2024-12-15T15:00:05  Hello doctor,\\nMy mother was diagnosed with st...   \n",
       "1  2024-12-15T14:00:04  Hello doctor,Last night I went for dinner and ...   \n",
       "2  2024-12-15T13:00:04  Hi doctor,I am a 59-year-old male and a nonsmo...   \n",
       "3  2024-12-15T12:00:04  Hi doctor,\\nI am a 35-year-old male. My height...   \n",
       "4  2024-12-15T11:00:04  Hi doctor,My sister delivered a baby one month...   \n",
       "\n",
       "                                              Answer  \n",
       "0  Hello,\\nWelcome to icliniq.com.\\nI can underst...  \n",
       "1  Hello,\\nWelcome to icliniq.com.\\nI read your q...  \n",
       "2  Hi,\\nWelcome to icliniq.com.\\nI have read your...  \n",
       "3  Hi,\\nWelcome to icliniq.com.\\nI have read your...  \n",
       "4  Hi,\\nWelcome to icliniq.com.\\nI have gone thro...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"icliniq_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615db405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello doctor,\\nMy mother was diagnosed with stage 3 breast cancer, which was completely removed. However, two years later, she experienced metastasis that spread to the brain. Now, the cancer has spread to the liver, and she has developed jaundice. Her bilirubin levels have increased from 7.5 to 10. Is there anything that can be done with medication? The common bile duct (CBD) is not dilated; she also has ascites, for which a combination of Spironolactone and Torasemide will be administered. Can anything be done to manage the jaundice?\\nKindly suggest.', 'Hello,\\nWelcome to icliniq.com.\\nI can understand your concern.\\nI have reviewed your reports and your query. To better assist, I would like to know a few things:\\n\\n\\nWhat treatments has she received so far?\\n\\n\\nWhat is the hormonal status of the tumor?\\n\\n\\nHow is her overall condition?\\n\\n\\nFrom what I can see, she has already received Palbociclib and Capecitabine, which suggests the tumor might be hormone receptor-positive and HER2-negative. In this case, treatment options are limited. If the tumor is triple-negative breast cancer (TNBC), we could consider a low-dose single-agent Carboplatin.\\nI understand this is a challenging situation. If her bilirubin levels and general condition are not optimal, chemotherapy options are limited. If you can provide a summary of her prior treatments, I will be able to offer more specific guidance. In the meantime, I recommend supportive care such as proper nutrition, pain management, and physiotherapy. These can help improve her quality of life.\\nI hope this helps.\\nLet me know if you have any other concerns.\\nThank you.'), ('Hello doctor,Last night I went for dinner and at a table nearby a lady was using a toothpick. When she put it out of her mouth I saw that about half of the toothpick had a reddish colour. I do not know if that was blood. Then the waiter came to take her order. The same waiter served me my dishes some minutes later. I did touch a dish at some point and a couple of hours later scratched my hand a bit. Is there a chance that if she had HIV she passed it to the waiter’s hand by talking, then the waiter passed it to my dish and this ended up in a scratch and infecting me?Kindly help.', 'Hello,\\nWelcome to icliniq.com.\\nI read your query and understand your concern.\\nThere are no reports of transmission of HIV (human immunodeficiency virus) by these modes. In my opinion the chances of transmission of HIV by given description is nil. There is no need to worry about HIV by this act.\\nI hope this helps.\\nRegards.'), ('Hi doctor,I am a 59-year-old male and a nonsmoker. Earlier diagnosed with mucus retention cyst of the larynx. Cetrizine and Ambroxol tablets were prescribed. No post-nasal drip was reported, and no evidence of GERD.Can a recurrent hoarseness of voice (once in four to five months) indicate cancer?Please help.Thank you.', 'Hi,\\nWelcome to icliniq.com.\\nI have read your query and understand your concern.\\nPlease revert to the following question.\\nWas any surgical procedure done for the cyst?\\nThank you.'), ('Hi doctor,\\nI am a 35-year-old male. My height is five feet and eight inches, and I weigh 194.007 pounds.\\nI have been suffering from Acid reflux GERD for the past five to six years and have been taking Pantocid-IT combination tablets for five years now (one tablet per day).I cannot stop the tablets due to recurring symptoms.\\n\\n\\nIs it okay to continue using the tablet Pantocid-IT for many years?\\n\\n\\nDoes it have any side effects?\\n\\n\\nDo I need to switch to any other tablets?\\n\\n\\nI heard the kidney may get affected if it continues for multiple years.\\nPlease advise.\\nThanks.', 'Hi,\\nWelcome to icliniq.com.\\nI have read your query and understand your concern.\\nYes, you can continue to take the Pantocid-IT tab as you are taking now. The PPI (proton pump inhibitor) group of drugs is very safe. Many people have taken these drugs for more than 15 years without any problems. The major side effects are hypocalcemia and osteoporosis and the risk of increased gastrointestinal infections.\\nI hope I have answered your queries. If you have more queries, do not hesitate to contact me.\\nThank you.'), ('Hi doctor,My sister delivered a baby one month ago. She is 33 years old and this is her first baby. She was having type 2 diabetes while she was pregnant. But now after delivery, she is free from the disease. She was taking insulin injections during delivery, but now after delivery her blood sugar level is normal and the doctor asked her not to take medication.Will this be permanent or should she check it monthly once or once in six months to reassure? Can type 2 diabetes in pregnant women be cured completely after delivery? Please help.Thank you.', 'Hi,\\nWelcome to icliniq.com.\\nI have gone through your query and understand your concern.\\nGestational diabetes is diagnosed during pregnancy and usually resolves after delivery. It occurs when the body cannot produce enough insulin to manage blood sugar levels. Pre-existing type 2 diabetes means the woman had diabetes before becoming pregnant and may need more intensive management during pregnancy.\\nPostpartum changes:\\n\\n\\nMany women who have gestational diabetes find that their blood sugar levels return to normal after childbirth. This can be due to hormonal changes, weight loss, and increased physical activity associated with caring for a newborn.\\n\\n\\nMonitoring blood sugar levels: Even if your sister’s blood sugar levels are currently normal, she should be vigilant about monitoring her health. It is recommended to have her blood sugar levels checked:\\n\\n\\n6 to 12 weeks postpartum: This is crucial to confirm that her blood sugar levels have returned to normal. Depending upon the results, further frequency of blood sugar monitoring will be assessed.\\n\\n\\nIncreased risk of future diabetes: Women who have gestational diabetes are at an increased risk of developing type 2 diabetes later in life, with studies suggesting that the risk is around 50 percent over the next 20 years. Therefore, ongoing monitoring is important.\\nStrategies to manage diabetes effectively:\\n\\n\\nIf your sister lost weight after delivery, it can significantly impact her blood sugar levels and reduce the risk of developing type 2 diabetes later on.\\n\\n\\nExercise: Aim for at least 150 minutes of moderate aerobic activity weekly.\\n\\n\\nRegularly follow up with the treating doctor with a blood glucose monitoring chart, to ensure correct adjustments to the advised treatment.\\n\\n\\nA balanced and nutritious diabetic diet shall be consumed. Dietician help should be taken to get a personalized diet chart.\\n\\n\\nEducate you and your family members about diabetes.\\n\\n\\nScreen for long-term complications including effects on eyes, heart, and kidneys. Consult specialist doctors for their individualized screening.\\n\\n\\nKeep a consistent schedule for checking your blood sugar, especially after meals, to understand how different foods affect your levels.\\n\\n\\nWhile it is wonderful that your sister is currently free from diabetes, she needs to remain proactive about her health. Regular check-ups, a healthy lifestyle, and monitoring her blood sugar levels will help her maintain her health in the long run. Her healthcare provider can give her personalized advice based on her specific situation, so keeping in touch with them is crucial.\\nI hope I have answered your question.\\nLet me know if I can assist you further.\\nThank you.')]\n",
      "48457\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for i in range(len(df)):\n",
    "    input_text = df.iloc[i][\"Question\"]\n",
    "    output_text = df.iloc[i][\"Answer\"]\n",
    "    pairs.append((input_text, output_text))\n",
    "print(pairs[:5])  # Show first 5 pairs\n",
    "print(len(pairs))  # Show total number of pairs\n",
    "pairs = pairs[:10000]  # Limit to first 1000 pairs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39c95887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b119f00bda343f99056a12c82dee021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Example dialogue pairs\n",
    "# Replace this with your real (input, response) pairs\n",
    "# data = [\n",
    "#     {\"input\": \"chat: Hello!\", \"output\": \"Hi there!\"},\n",
    "#     {\"input\": \"chat: How are you?\", \"output\": \"I'm doing well, thank you.\"},\n",
    "#     {\"input\": \"chat: What's your name?\", \"output\": \"I'm a chatbot.\"}\n",
    "#     # Add more pairs from Cornell/PersonaChat here\n",
    "# ]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "data= pairs \n",
    "# Clean the data\n",
    "data = [(clean_text(src), clean_text(tgt)) for src, tgt in data]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['input', 'output']\n",
    "df['input'] = \"chat: \" + df['input']  # Prefix for T5\n",
    "df['output'] = df['output']  # No prefix for output\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "def preprocess(example):\n",
    "    input_enc = tokenizer(example['input'], truncation=True, padding='max_length', max_length=32)\n",
    "    target_enc = tokenizer(example['output'], truncation=True, padding='max_length', max_length=32)\n",
    "    input_enc['labels'] = target_enc['input_ids']\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02bda47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3a4c54d146449982812afb767415a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved to data/processed/icliniq_t5_dataset\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenized dataset\n",
    "output_dir = \"data/processed/icliniq_t5_dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tokenized_dataset.save_to_disk(output_dir)\n",
    "print(f\"Tokenized dataset saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba0222d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# shoew the dataset along with cols\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c63872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 9025\n",
      "Validation dataset size: 475\n",
      "Test dataset size: 500\n"
     ]
    }
   ],
   "source": [
    "train_test = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_val = train_test['train'].train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_val['train']\n",
    "val_dataset = train_val['test']\n",
    "test_dataset = train_test['test']\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
